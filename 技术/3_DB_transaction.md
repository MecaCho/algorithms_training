# 一、事务    
事务是由一组SQL语句组成的逻辑处理单元，是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。

事务具有以下4个属性，通常简称为事务的ACID属性:
- 原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。
- 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。
- 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。
- 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 　可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。
MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。

这几个特性不是一种平级关系：
只有满足一致性，事务的执行结果才是正确的。
在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。
在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。
事务满足持久化是为了能应对数据库奔溃的情况。


# 二、并发一致性问题

## 1、更新丢失(Lost Update)

T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。
例如，两个程序员修改同一java文件。每程序员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖前一个程序员所做的更改。
如果在一个程序员完成并提交事务之前，另一个程序员不能访问同一文件，则可避免此问题。

## 2、脏读
一句话：事务B读取到了事务A已修改但尚未提交的的数据，还在这个数据基础上做了操作。
此时，如果A事务回滚Rollback，B读取的数据无效，不符合一致性要求。

解决办法: 把数据库的事务隔离级别调整到 READ_COMMITTED
T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。

## 3、不可重复读(Non-Repeatable Reads)
在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。
那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，
这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。

一句话：一个事务范围内两个相同的查询却返回了不同数据。

同时操作，事务1分别读取事务2操作时和提交后的数据，读取的记录内容不一致。
不可重复读是指在同一个事务内，两个相同的查询返回了不同的结果。

解决办法: 如果只有在修改事务完全提交之后才可以读取数据，则可以避免该问题。把数据库的事务隔离级别调整到REPEATABLE_READ

T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。

## 4、幻读    
一个事务T1按相同的查询条件重新读取以前检索过的数据，却发现其他事务T2插入了满足其查询条件的新数据，这种现象就称为“幻读”。（和可重复读类似，但是事务 T2 的数据操作仅仅是插入和删除，不是修改数据，读取的记录数量前后不一致）

一句话：事务A 读取到了事务B提交的新增数据，不符合隔离性。

解决办法: 如果在操作事务完成数据处理之前，任何其他事务都不可以添加新数据，则可避免该问题。
把数据库的事务隔离级别调整到 SERIALIZABLE_READ。

T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。


# 三、事务隔离级别    
"脏读"、"不可重复读"和"幻读"，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。

数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。

MYSQL常看当前数据库的事务隔离级别：show variables like 'tx_isolation';

## 1、读未提交 (Read Uncommitted)

最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。

## 2、读已提交 (Read Committed)

被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。

## 3、可重复读(Repeatable Read)

所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。(引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。)

MYSQL默认是REPEATABLE-READ 。

## 4、串行化(Serializable)
所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。

## 5、总结

读未提交: 一个事务还没提交时，它做的变更就能被别的事务看到。
读提交: 一个事务提交之后，它做的变更才会被其他事务看到。
可重复读 : 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
串行化: 顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。
四个级别逐渐增强，每个级别解决一个问题，事务级别越高，性能越差，大多数环境(Read committed 就可以用了)


# 并发控制

通过并发控制保证数据一致性的常见手段有：
1。锁（Locking）
2。数据多版本（Multi Versioning）

## 锁
普通锁，被使用最多：
(1)操作数据前，锁住，实施互斥，不允许其他的并发任务操作；
(2)操作完成后，释放锁，让其他任务执行；
简单的锁住太过粗暴，连“读任务”也无法并行，任务执行过程本质上是串行的。

### 共享锁vs排他锁

- 共享锁（Share Locks，记为S锁），读取数据时加S锁
    共享锁之间不互斥，简记为：读读可以并行
- 排他锁（eXclusive Locks，记为X锁），修改数据时加X锁
    排他锁与任何锁互斥，简记为：写读，写写不可以并行

对应到数据库，可以理解为，写事务没有提交，读相关数据的select也会被阻塞。
一旦写数据的任务没有完成，数据是不能被其他任务读取的，这对并发度有较大的影响。
即使写任务没有完成，其他读任务也可能并发，这就引出了数据多版本。

## 数据多版本

数据多版本是一种能够进一步提高并发的方法，它的核心原理是：
（1）写任务发生时，将数据克隆一份，以版本号区分；
（2）写任务操作新克隆的数据，直至提交；
（3）并发读任务可以继续读取旧版本的数据，不至于阻塞；

数据多版本，通过“读取旧版本数据”能够极大提高任务的并发度。

## 总体思路

提高并发的演进思路，就在如此：
1。普通锁，本质是串行执行
2。读写锁，可以实现读读并发
3。数据多版本，可以实现读写并发

## redo，undo，回滚段

### redo日志

redo日志用于保障，已提交事务的ACID特性。

- 随机写优化为顺序写
数据库事务提交后，必须将更新后的数据刷到磁盘上，以保证ACID特性。
磁盘随机写性能较低，如果每次都刷盘，会极大影响数据库的吞吐量。
优化方式是，将修改行为先写到redo日志里（此时变成了顺序写），再定期将数据刷到磁盘上，这样能极大提高性能。

假如某一时刻，数据库崩溃，还没来得及刷盘的数据，在数据库重启后，会重做redo日志里的内容，
以保证已提交事务对数据产生的影响都刷到磁盘上。

### undo日志

undo日志用于保障，未提交事务不会对数据库的ACID特性产生影响。

为什么用undo：
数据库事务未提交时，会将事务修改数据的镜像（即修改前的旧版本）存放到undo日志里，
当事务回滚时，或者数据库崩溃时，可以利用undo日志，即旧版本数据，撤销未提交事务对数据库产生的影响。

对于insert操作，undo日志记录新数据的PK(ROW_ID)，回滚时直接删除；----自增ID不连续问题
对于delete/update操作，undo日志记录旧数据row，回滚时直接恢复；

### 回滚段

存储undo日志的地方，是回滚段。

# 多版本并发控制（Multi Version Concurrency Control, MVCC）

行锁，并发，事务回滚等多种特性都和MVCC相关。

MVCC就是通过“读取旧版本数据”来降低并发事务的锁冲突，提高任务的并发度。

## 影响

旧版本数据存储在哪里？
存储旧版本数据，对MySQL和InnoDB原有架构是否有巨大冲击？
(1)旧版本数据存储在回滚段里；
(2)对MySQL和InnoDB原有架构体系冲击不大；

InnoDB的内核，会对所有row数据增加三个内部属性：
(1)DB_TRX_ID，6字节，记录每一行最近一次修改它的事务ID；
(2)DB_ROLL_PTR，7字节，记录指向回滚段undo日志的指针；
(3)DB_ROW_ID，6字节，单调递增的行ID；

## 快照读

普通的select语句都是快照读

这种一致性不加锁的读（Consistent Nonlocking Read），就是InnoDB并发如此之高的核心原因之一。

显式加锁，非快照读是指：
select * from t where id>2 lock in share mode;
select * from t where id>2 for update;


# 如何避免长事务对业务的影响？

这个问题，我们可以从应用开发端和数据库端来看。

## 开发端    
首先，从应用开发端来看：
1. 确认是否使用了 set autocommit=0。
这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。
一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。
2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。
我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。
这种只读事务可以去掉。
3. 业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，
来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）

## 数据库端    
其次，从数据库端来看：
1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；
2. Percona 的 pt-kill 这个工具不错，推荐使用；
3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；
4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。
如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。


# 基本

## 关系型数据库的三大范式：

```
第一范式：只要是关系型数据库的表，都满足第一范式。
性质：第一范式的数据表中的所有字段都是单一属性，不可分割。
第二范式：不可使用组合键，确保唯一主键。
第三范式：要求数据表中不存在非关键字段对任一候选关键字段的传递函数依赖，表分开。
```

## Innodb和MySIAM两种引擎的区别

```
MYIASM:管理非事务表，提供高速存储和检索，以及全文搜索能力，如果在应用中执行大量的select操作，应选择MYIASM引擎
Innodb:用于事务处理，具有ACID事务支持等特性，如果在应用中执行大量的insert和update操作，应选择innodb引擎。
```

## 什么是主键?什么是外键?

```
主键:表格里的(一个或多个)字段，只用来定义表格里的行;主键里的值总是唯一的。
外键:一个用来建立两个表格之间关系的约束。
这种关系一般都涉及一个表格里的主键字段与另外一个表格(尽管可能是同一个表格)里的一系列相连的字段。那么这些相连的字段就是外键。

主键在本表中是唯一的、不可为空的，外键可以重复可以唯空；
外键和另一张表的主键关联，不能创建对应表中不存在的外键。
```

# 事务和锁

## 1.什么是事务

```
事务：事务是并发控制的基本单元，事务是一个操作序列，要么都执行，要么都不执行，
是一个不可分割的工作单位，事务是维护数据库一致性的单位。
锁：在所以的DBMS中，锁是实现事务的关键，锁可以保证事务的完整性和并发性。
与现实生活中锁一样，它可以使某些数据的拥有者，在某段时间内不能使用某些数据或数据结构。当然锁还分级别的。

四个ACID基本性质：

1.原子性：要么都执行，要么都不执行。
2.一致性：合法的数据才可以被写入。
3.隔离性：允许多个用户并发访问。
4.持久性：事务结束后，事务处理的结果必须得到固化。即一旦提交，对数据库改变是永久的。

事物的语句：

1.开始事务：BEGIN TRANSACTION
2.提交事务：COMMIT TRANSACTION
3.回滚事务：ROLLBACK TRANSACTION
```


## 锁机制

```
加锁是为了实现并发控制。数据库是一个多用户资源，
若对并发控制不加控制会读取和存储不正确的数据，破坏数据的一致性（脏读，不可重复读，幻读等）可能会产生死锁。
锁机制保证在一个事务释放锁之前其他事务不可以进行修改。

    行级锁:
    表级锁:
    悲观锁：事务每次操作之前假设有其他事务会修改需访问的数据，会要求上锁。
    乐观锁：事务每次操作之前假设没有其他事务会修改需访问的数据，不会要求上锁。
    
    共享锁：对某一资源加共享锁，自身可以读该资源，其他人也可以读该资源
    共享锁（S锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的
                    事务只能读数据，不能修改数据。
    排他锁（X锁）：如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务
                    既能读数据，又能修改数据。
    共享锁下其它用户可以并发读取，查询数据。但不能修改，增加，删除数据。资源共享。
```

## 四种隔离级别：

```
1，未提交读：读数据时不会检查使用任何锁。
2，已提交读：只读取提交的数据并等待其他事务释放锁。
3，可重复读：会保持共享锁到事务结束。
4，可序列化：不仅会锁定影响的数据，还会锁定这个范围
```


# 运维

## mysql的四种日志：

```
1.错误日志：Error Log.记录mysql运行过程ERROR，WARING等信息，系统出错或某条记录出问题可查看ERROR日志。
2.日常运行日志：General query log记录mysql中每条请求数据。
3.二进制日志：Binary log，包含一些事件，数据库的改动等。
4.慢查询日志：slow query log，用于mysql的性能调优。
```

## 慢查询：

```
1，开启慢查询，可以让mysql记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好地优化数据库系统的性能。
slow_query_log 慢查询开启状态
slow_query_log_file 慢查询日志的存放位置
long_query_time查询超过多少秒才记录

2，可以通过设置全局变量的方法设定：
例如：set gloable slow_query_long on   开启慢查询状态
service mysqld restart 即可

3，查询对应值：
show variables like'slow_query%';
show variables like 'long_query_time';

4，测试慢查询是否正确开启：
select sleep(2);   执行慢查询语句，查看是否有对应的慢查询日志生成。
```

## 如何优化数据库，如何提高数据库的性能？

```
1)给数据库做索引，合理的索引能立即显著地提高数据库整个系统的性能。

2)在适当的情况下，尽可能的用存储过程而不是SQL查询。因为前者已经过了预编译，运行速度更快。

3)优化查询语句，通过高性能的查询语句提高数据库的性能。
```


